{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6EclfMI3kWf",
        "outputId": "09bc4fc7-9bfb-4371-b890-8a4a940217d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class distribution before balancing: Counter({0: 183964, 1: 183405})\n",
            "Class distribution after balancing: Counter({0: 183964, 1: 183964})\n",
            "Dataset size after subsampling: 183964\n",
            "Train shape: (132453, 2), Validation shape: (14718, 2), Test shape: (36793, 2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at HuggingFaceTB/SmolLM-135M and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-930322f926e2>:149: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = amp.GradScaler()  # For mixed precision training\n",
            "Training Epoch 1:   0%|          | 0/4140 [00:00<?, ?it/s]<ipython-input-4-930322f926e2>:159: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast():\n",
            "Training Epoch 1: 100%|██████████| 4140/4140 [26:40<00:00,  2.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2, Training Loss: 0.1603\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "<ipython-input-4-930322f926e2>:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9551\n",
            "\n",
            "Classification Report:\n",
            "                  precision    recall  f1-score   support\n",
            "\n",
            "Non-Plagiarized       0.96      0.95      0.95      7347\n",
            "    Plagiarized       0.95      0.96      0.96      7371\n",
            "\n",
            "       accuracy                           0.96     14718\n",
            "      macro avg       0.96      0.96      0.96     14718\n",
            "   weighted avg       0.96      0.96      0.96     14718\n",
            "\n",
            "Epoch 1/2, Validation Accuracy: 0.9551\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Epoch 2:   0%|          | 0/4140 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "<ipython-input-4-930322f926e2>:159: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast():\n",
            "Training Epoch 2: 100%|██████████| 4140/4140 [26:40<00:00,  2.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/2, Training Loss: 0.0758\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "<ipython-input-4-930322f926e2>:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9571\n",
            "\n",
            "Classification Report:\n",
            "                  precision    recall  f1-score   support\n",
            "\n",
            "Non-Plagiarized       0.97      0.95      0.96      7347\n",
            "    Plagiarized       0.95      0.97      0.96      7371\n",
            "\n",
            "       accuracy                           0.96     14718\n",
            "      macro avg       0.96      0.96      0.96     14718\n",
            "   weighted avg       0.96      0.96      0.96     14718\n",
            "\n",
            "Epoch 2/2, Validation Accuracy: 0.9571\n",
            "Training complete!\n",
            "\n",
            "Evaluating the model on test set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "<ipython-input-4-930322f926e2>:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9600\n",
            "\n",
            "Classification Report:\n",
            "                  precision    recall  f1-score   support\n",
            "\n",
            "Non-Plagiarized       0.97      0.95      0.96     18566\n",
            "    Plagiarized       0.95      0.97      0.96     18227\n",
            "\n",
            "       accuracy                           0.96     36793\n",
            "      macro avg       0.96      0.96      0.96     36793\n",
            "   weighted avg       0.96      0.96      0.96     36793\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "import torch.cuda.amp as amp  # For mixed precision training\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Step 1: Load and Preprocess the Dataset\n",
        "def load_txt_file(file_path):\n",
        "    # Load the dataset with specified column names\n",
        "    df = pd.read_csv(file_path, delimiter='\\t', header=None, names=['sentence1', 'sentence2', 'label'])\n",
        "\n",
        "    # Drop rows with missing values\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "    # Filter for binary labels (0 and 1)\n",
        "    df = df[df['label'].isin([0, 1])]\n",
        "\n",
        "    # Concatenate sentence1 and sentence2 with [SEP]\n",
        "    df['text'] = df['sentence1'] + \" [SEP] \" + df['sentence2']\n",
        "\n",
        "    # Check class distribution\n",
        "    print(\"Class distribution before balancing:\", Counter(df['label']))\n",
        "\n",
        "    # Oversample minority class to balance the dataset (default behavior: equalize classes)\n",
        "    oversampler = RandomOverSampler(random_state=42)  # Removed sampling_strategy=0.5\n",
        "    X_resampled, y_resampled = oversampler.fit_resample(df[['text']], df['label'])\n",
        "    df = pd.DataFrame({'text': X_resampled['text'], 'label': y_resampled})\n",
        "\n",
        "    print(\"Class distribution after balancing:\", Counter(df['label']))\n",
        "\n",
        "    # Subsample the dataset to reduce training time (e.g., 50%)\n",
        "    df = df.sample(frac=0.5, random_state=42)\n",
        "    print(f\"Dataset size after subsampling: {len(df)}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = text.replace('\\d+', '')  # Remove numbers\n",
        "    return text\n",
        "\n",
        "# Step 2: Prepare the Dataset Class\n",
        "class PlagiarismDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, max_length=128):  # Reduced to 128\n",
        "        self.df = df\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        row = self.df.iloc[index]\n",
        "        text = str(row['text']) if not pd.isna(row['text']) else \"\"\n",
        "        inputs = self.tokenizer(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        label = torch.tensor(row['label'], dtype=torch.long)\n",
        "\n",
        "        return {\n",
        "            'input_ids': inputs['input_ids'].squeeze(0),\n",
        "            'attention_mask': inputs['attention_mask'].squeeze(0),\n",
        "            'label': label\n",
        "        }\n",
        "\n",
        "# Custom collate function for DataLoader\n",
        "def collate_fn(batch):\n",
        "    input_ids = torch.stack([item['input_ids'] for item in batch])\n",
        "    attention_masks = torch.stack([item['attention_mask'] for item in batch])\n",
        "    labels = torch.stack([item['label'] for item in batch])\n",
        "    return {\n",
        "        'input_ids': input_ids,\n",
        "        'attention_mask': attention_masks,\n",
        "        'label': labels\n",
        "    }\n",
        "\n",
        "# Step 3: Load and Split the Dataset\n",
        "file_path = '/content/train_snli.txt'\n",
        "df = load_txt_file(file_path)\n",
        "df['text'] = df['text'].apply(preprocess_text)\n",
        "\n",
        "# Split into train, validation, and test sets\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42)\n",
        "\n",
        "print(f\"Train shape: {train_df.shape}, Validation shape: {val_df.shape}, Test shape: {test_df.shape}\")\n",
        "\n",
        "# Step 4: Initialize Tokenizer and Model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM-135M\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"HuggingFaceTB/SmolLM-135M\", num_labels=2)\n",
        "\n",
        "# Add padding token and resize embeddings\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# Step 5: Create Datasets and DataLoaders\n",
        "train_set = PlagiarismDataset(train_df, tokenizer, max_length=128)\n",
        "valid_set = PlagiarismDataset(val_df, tokenizer, max_length=128)\n",
        "test_set = PlagiarismDataset(test_df, tokenizer, max_length=128)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_set,\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=4\n",
        ")\n",
        "valid_loader = DataLoader(\n",
        "    valid_set,\n",
        "    batch_size=32,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=4\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    test_set,\n",
        "    batch_size=32,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=4\n",
        ")\n",
        "\n",
        "# Step 6: Setup Device and Model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Step 7: Define Training and Evaluation Functions\n",
        "def train_model(model, train_loader, val_loader, optimizer, loss_fn, epochs=2):\n",
        "    scaler = amp.GradScaler()  # For mixed precision training\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\"):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            with amp.autocast():\n",
        "                outputs = model(input_ids, attention_mask=attention_mask)\n",
        "                loss = loss_fn(outputs.logits, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        val_accuracy, _ = evaluate_model(model, val_loader)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Validation Accuracy: {val_accuracy:.4f}\")\n",
        "    print(\"Training complete!\")\n",
        "\n",
        "def evaluate_model(model, data_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            with amp.autocast():\n",
        "                outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            predictions = torch.argmax(outputs.logits, dim=1)\n",
        "\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predictions.cpu().numpy())\n",
        "\n",
        "            correct += (predictions == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    accuracy = correct / total\n",
        "    report = classification_report(all_labels, all_predictions, target_names=['Non-Plagiarized', 'Plagiarized'])\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(\"\\nClassification Report:\\n\", report)\n",
        "    return accuracy, report\n",
        "\n",
        "# Step 8: Train the Model\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "loss_fn = CrossEntropyLoss()\n",
        "train_model(model, train_loader, valid_loader, optimizer, loss_fn, epochs=2)\n",
        "\n",
        "# Step 9: Evaluate on Test Set\n",
        "print(\"\\nEvaluating the model on test set...\")\n",
        "test_accuracy, test_report = evaluate_model(model, test_loader)"
      ]
    }
  ]
}